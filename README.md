# ðŸ‹ï¸â€â™‚ï¸ HyroxGPT: Personalized Feedback Generator for Hyrox Athletes

*KeepCoding Full-Stack AI Bootcamp III*

**Author:** Mirellys Arteta Davila

**Module:** Large Language Models (LLMs)

ðŸ’¡ What It Does

HyroxGPT is an intelligent system that analyzes race performance data from Hyrox competitions and generates personalized feedback for athletes. It combines classical machine learning with LLM fine-tuning techniques to deliver insights such as:
	â€¢	Whether the athlete over- or under-performed vs expected
	â€¢	What their cluster/group performance looks like
	â€¢	Recommendations for improvement

 ðŸ—‚ï¸ Project Structure

ðŸ“Š 1.Hyrox_ML/ â€” Machine Learning Performance Analysis
ðŸ’»Â File:
	â€¢	Hyrox_ML.ipynb

 This notebook contains the first stage of the project where:
 
	â€¢	Raw Hyrox results were preprocessed and converted into numerical features.
	â€¢	Feature engineering was applied (z-scores, MinMax scaling, clustering).
	â€¢	Models (Linear Regression and LightGBM) were trained to predict total race time.
	â€¢	An intermediate performance_feedback column was generated, used later for LLM fine-tuning.

ðŸ¤– 2.Fine-Tuning/ â€” LLM Fine-Tuning (GPT-2 and Mistral-7B + LoRA)
ðŸ’»Â Files:
	â€¢	Hyrox_Fine_tuning_GPT2.ipynb
	â€¢	Hyrox_Fine_tuning_Mistral7B_LoRA.ipynb

This folder contains attempts at fine-tuning language models to generate more human-like feedback:
	â€¢	GPT-2 was used in the first iteration. It was easy to train but limited in expressiveness and generalization.
	â€¢	Mistral-7B + LoRA later explored to improve output quality, but training was interrupted due to limited Colab GPU time and missing configuration (offload folder).
	â€¢	Instruction-style prompts were created from the ML feedback for use in model training.

 ðŸ“Ž Files in Root
	â€¢	Presentation.pdf: Summary slides of the project and results.
	â€¢	LICENSE: Open source license.
	â€¢	README.md: This file.

 ðŸ“‚ Resources & Data
	â€¢	hyrox_data_1.csv: Cleaned dataset used for ML model training and feedback generation.
	â€¢	hyrox_json_instructed.jsonl: Instruction-formatted dataset used for GPT-2 and Mistral fine-tuning.
	â€¢	hyrox_split_dataset/: Contains the train/val/test splits of the instruction dataset.
	â€¢	hyrox_gpt2_model/: Fine-tuned GPT-2 model artifacts (model weights, tokenizer config, etc.) --> ðŸ¤— https://huggingface.co/Syllerim/gpt2-hyrox
 
